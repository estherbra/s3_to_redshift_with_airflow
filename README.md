<div style="font-size:20px">
  <h1>s3 to redshift with Apache Airflow </h1>
</div>

# Sobre o Projeto

<br/>

## üåê Overview

Esse projeto foi feito com a linguagem de programa√ß√£o Python, utilizando o AWS SDK para Python Boto3 para facilitar a integra√ß√£o do script com o servi√ßo de Cloud Storage da AWS (Amazon s3), al√©m das bibliotecas json, pandas, os e glob.
Foi utilizado o orquestrador de fluxos Apache Airflow para a cria√ß√£o de um trigger com hor√°rio pr√©-definido e a ferramenta de administra√ß√£o de banco de dados multiplataforma Dbeaver, conectada ao Data Warehouse Amazon Redshift para execu√ß√£o de consultas SQL. O trigger √© disparado num hor√°rio espec√≠fico e copia todos os dados de uma vez do s3 Bucket, criando uma tabela chamada "landing_table" no Data Warehouse.
Por fim, o Data Warehouse √© conectado √° ferramenta Power BI, que consiste em um servi√ßo de an√°lise de neg√≥cios e fornece uma visualiza√ß√£o da tabela credit_per_day.

<br/>

### Arquitetura do projeto:

![arquitetura do projeto](https://i.imgur.com/8KYlwVY.png)

## Passo a passo
1. A primeira etapa consiste em ajustar algumas permiss√µes do bucket, como a inclus√£o da action "s3:PutObject" na pol√≠tica (caso necess√°rio), para em seguida fazer a ingest√£o dos 50 arquivos semi-estruturados em um Bucket da Amazon s3 atrav√©s do script de Jupyter Notebook "case-keycash.ipynb" na pasta "passo 1", no qual tamb√©m √© realizada a convers√£o do tipo json para csv por meio da biblioteca pandas.

![pol√≠tica bucket](https://i.imgur.com/XZgzKoP.png)

![objetos s3 bucket](https://i.imgur.com/PD0TF6N.png)

2. Na segunda etapa, √© criado o cluster redshift abaixo e conectado ao Dbeaver atrav√©s do seu endpoint.

![Redshift Cluster](https://i.imgur.com/nBDTkHs.png)

![conex√£o Dbeaver](https://i.imgur.com/qlR53y3.png)

3. Download do Apache Airflow e cria√ß√£o da DAG escrita em Python, a qual se conecta com o Bucket e dispara o trigger que envia os arquivos contidos nele para a tabela "landing_table" (criada na pr√≥pria DAG) no Amazon Redshift em um hor√°rio pr√©-determinado (19h30 do dia 21 de fevereiro) atrav√©s de CRON expression '30 19 21 02 mon'. A DAG file est√° na pasta "passo 2".

![DAG](https://i.imgur.com/CMWLAPg.png)

![DAG](https://i.imgur.com/F9fz4Z7.png)

4. A partir da tabela landing_table foi criada uma nova chamada credit_per_day, agregando o somat√≥rio de Cr√©dito Solicitado (credito_solicitado) por dia (data_solicitada), considerando que alguns clientes podem ter solicitado cr√©dito mais de uma vez e apenas a solicita√ß√£o mais recente √© v√°lida. √â necess√°rio que se rode os dois scripts SQL dispon√≠veis na pasta "passo 3", na seguinte ordem: "landing_table.sql" --> "credit_per_day.sql". O primeiro se faz necess√°rio para a corre√ß√£o do tipo de dado da coluna data_solicitacao e o segundo para cria√ß√£o de nova tabela e inser√ß√£o e consolida√ß√£o de dados.

![landing_table](https://i.imgur.com/E5gxn3L.png)

![credit_per_day](https://i.imgur.com/NMnCmeL.png)

5. Por fim, com o objetivo de criar uma visualiza√ß√£o da tabela credit_per_day, o Data Warehouse foi conectado √° ferramenta Power BI, resultando no seguinte:

![power bi](https://i.imgur.com/yC8MTjJ.png)


# Configurando o ambiente

### Requerimentos

- Python version 3.9
- Apache Airflow version 2.2.3
- Pandas version 1.1.3
- Dbeaver version 21.3.3


 <br/>

### Instalando as dependencias

```
pip install boto3
pip install pandas
```

<br/>

