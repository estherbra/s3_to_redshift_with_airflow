<div style="font-size:20px">
  <h1>s3 to redshift with Apache Airflow - Case Keycash</h1>
</div>

# Sobre o Projeto

<br/>

## üåê Overview

Esse projeto foi feito com a linguagem de programa√ß√£o Python, utilizando o AWS SDK para Python Boto3 para facilitar a integra√ß√£o do script com o servi√ßo de Cloud Storage da AWS (Amazon s3), al√©m das bibliotecas json, pandas, os e glob.
Foi utilizado o orquestrador de fluxos Apache Airflow para a cria√ß√£o de um trigger com hor√°rio pr√©-definido e a ferramenta de administra√ß√£o de banco de dados multiplataforma Dbeaver, conectada ao Data Warehouse Amazon Redshift para execu√ß√£o de queries. O trigger √© disparado num hor√°rio espec√≠fico e copia todos os dados de uma vez do s3 Bucket para uma tabela chamada "landing_table" no Data Warehouse.
Por fim, o Data Warehouse √© conectado √° ferramenta Power BI, que consiste em um servi√ßo de an√°lise de neg√≥cios e fornece uma visualiza√ß√£o da tabela credit_per_day.

<br/>

### Arquitetura do projeto:

![arquitetura do projeto](https://i.imgur.com/jepb5AO.png)

## Passo a passo 
1. A primeira etapa consiste em ajustar algumas permiss√µes do bucket, como a inclus√£o da action "s3:PutObject" na pol√≠tica (caso necess√°rio), para em seguida fazer a ingest√£o dos 50 arquivos semi-estruturados em um Bucket da Amazon s3 atrav√©s de um script Python, no qual tamb√©m √© realizada a convers√£o do tipo json para csv por meio da biblioteca pandas.

![pol√≠tica bucket](https://i.imgur.com/FgASFtB.png)

![objetos s3 bucket](https://i.imgur.com/UuJ2kER.png)

2. Na segunda etapa, √© criado o cluster redshift abaixo e conectado ao Dbeaver atrav√©s do seu endpoint.

![Redshift Cluster](https://i.imgur.com/Whq71se.png)

![conex√£o Dbeaver](https://i.imgur.com/zs6cVj3.png)

3. Download do Apache Airflow e cria√ß√£o da DAG escrita em Python, a qual se conecta com o bucket e dispara o trigger que envia os arquivos contidos nele para a tabela "landing_table" no Amazon Redshift em um hor√°rio pr√©-determinado (19h30 do dia 21 de fevereiro) atrav√©s de CRON expression '30 19 21 02 mon'. A DAG file est√° dispon√≠vel neste reposit√≥rio

![DAG](https://i.imgur.com/xMS8t4Q.png)

![DAG](https://i.imgur.com/KiGc6wG.png)

4. A partir da tabela landing_table foi criada uma nova chamada credit_per_day, agregando o somat√≥rio de Cr√©dito Solicitado (credito_solicitado) por dia (data_solicitada), considerando que alguns clientes podem ter solicitado cr√©dito mais de uma vez e apenas a solicita√ß√£o mais recente √© v√°lida. As consultas est√£o dispon√≠veis neste reposit√≥rio.

![landing_table](https://i.imgur.com/CsqbOtG.png)

![credit_per_day](https://i.imgur.com/5Jt4qdd.png)

5. Por fim, com o objetivo de criar uma visualiza√ß√£o da tabela credit_per_day, o Data Warehouse foi conectado √° ferramenta Power BI, resultando no seguinte:

![power bi](https://i.imgur.com/nU7NvGr.png)


# Configurando o ambiente

### Requerimentos

- Python version 3.9
- Apache Airflow version 2.2.3
- Pandas version 1.1.3
- Dbeaver version 21.3.3


 <br/>

### Instalando as dependencias

```
pip install boto3
pip install pandas
```

<br/>

